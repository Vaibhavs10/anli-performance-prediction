# -*- coding: utf-8 -*-
"""Alpha NLI - BERT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vCuX3PJ_tVt8rzEjue_20_A5Uo3SnWMz
"""

import csv
from csv import reader
import pandas as pd
import numpy as np

from datasets import load_dataset, load_metric

from transformers import AutoTokenizer
from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer

def parse_and_return_rows(file_path):
    with open(file_path, 'r', encoding='utf-8') as read_obj:
        # pass the file object to reader() to get the reader object
        csv_reader = reader(read_obj)
        # Pass reader object to list() to get a list of lists
        list_of_rows = list(csv_reader)
    return list_of_rows

train_list_of_rows = parse_and_return_rows("train.csv")
dev_list_of_rows = parse_and_return_rows("dev.csv")

def create_csv(combined_observation, hypothesis, label, file_path):
    rows = zip(combined_observation, hypothesis, label)
    with open(file_path, "w") as f:
        writer = csv.writer(f)
        writer.writerow(["observation", "hypothesis", "label"])
        for row in rows:
            writer.writerow(row)

def parse_dataloader(dataloader):
    processed_dataloader = []
    for entry in dataloader:
        data_dict = {}
        data_dict["observation"] = entry[1] + " " + entry[2]
        data_dict["hypothesis0"] = entry[3]
        data_dict["hypothesis1"] = entry[4]
        data_dict["label"] = int(entry[5]) - 1
        processed_dataloader.append(data_dict)
    return processed_dataloader

train = parse_dataloader(train_list_of_rows)

val = parse_dataloader(dev_list_of_rows)

pd.DataFrame(train).to_csv("train.csv", index=False)
pd.DataFrame(val).to_csv("val.csv", index=False)

datasets = load_dataset('csv', data_files={'train': 'train.csv',
                                           'validation': 'val.csv'})

model_checkpoint = "bert-base-uncased"
batch_size = 32

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)

ending_names = ["hypothesis0", "hypothesis1"]

def preprocess_function(examples):
    # Repeat each first sentence four times to go with the four possibilities of second sentences.
    first_sentences = [[context] * 2 for context in examples["observation"]]
    # Grab all second sentences possible for each context.
    second_sentences = [examples[end] for end in ending_names]
    second_sentences = [list(a) for a in zip(*second_sentences)]
    
    # Flatten everything
    first_sentences = sum(first_sentences, [])
    second_sentences = sum(second_sentences, [])
 
    # Tokenize
    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)
    # Un-flatten
    return {k: [v[i:i+2] for i in range(0, len(v), 2)] for k, v in tokenized_examples.items()}

encoded_datasets = datasets.map(preprocess_function, load_from_cache_file=False, batched=True)

model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)

args = TrainingArguments(
    "test-glue",
    evaluation_strategy = "epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,
    weight_decay=0.01,
    save_strategy="epoch"
)

from dataclasses import dataclass
from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy
from typing import Optional, Union
import torch

@dataclass
class DataCollatorForMultipleChoice:
    """
    Data collator that will dynamically pad the inputs for multiple choice received.
    """

    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None

    def __call__(self, features):
        label_name = "label" if "label" in features[0].keys() else "labels"
        labels = [feature.pop(label_name) for feature in features]
        batch_size = len(features)
        num_choices = len(features[0]["input_ids"])
        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]
        flattened_features = sum(flattened_features, [])
        
        batch = self.tokenizer.pad(
            flattened_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )
        
        # Un-flatten
        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}
        # Add back labels
        batch["labels"] = torch.tensor(labels, dtype=torch.int64)
        return batch

def compute_metrics(eval_predictions):
    predictions, label_ids = eval_predictions
    preds = np.argmax(predictions, axis=1)
    return {"accuracy": (preds == label_ids).astype(np.float32).mean().item()}

trainer = Trainer(
    model,
    args,
    train_dataset=encoded_datasets["train"],
    eval_dataset=encoded_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=DataCollatorForMultipleChoice(tokenizer),
    compute_metrics=compute_metrics,
)

trainer.train()