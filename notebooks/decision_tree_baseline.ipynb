{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0d7ec495c5f58bd4b22e9225a18398c3dcdacf9e275ce50867fb77cc7ed238bef",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "d7ec495c5f58bd4b22e9225a18398c3dcdacf9e275ce50867fb77cc7ed238bef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from collections import defaultdict \n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\utils\")\n",
    "    sys.path.append(module_path+\"\\\\models\")\n",
    "\n",
    "import vectorizer\n",
    "from decision_tree import BinaryDecisionTreeNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = vectorizer.parse_and_return_rows('../utils/data/processed_data/dev.csv')\n",
    "vocabulary, vocabulary_length = vectorizer.return_len_and_vocabulary(rows)\n",
    "word_to_index = vectorizer.create_token_index(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training instances with the first hypothesis\n",
    "training_instances  = [(row[1] + \" \" + row[2] + \" \" + row[3], 1 if row[5] == '1' else 0) for row in rows]\n",
    "\n",
    "# add training instances with the second hypothesis\n",
    "training_instances += [(row[1] + \" \" + row[2] + \" \" + row[4], 1 if row[5] == '2' else 0) for row in rows]\n",
    "\n",
    "# change the text into sparse incidence vectors\n",
    "vectorized_training_instances = [(vectorizer.sparse_incidence_vector(text, word_to_index), label) for (text, label) in training_instances]\n",
    "\n",
    "# convert from list of (vector, label) tuples into two separate lists\n",
    "[x, y] = [list(t) for t in zip(*vectorized_training_instances)]"
   ]
  }
 ]
}