{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0d7ec495c5f58bd4b22e9225a18398c3dcdacf9e275ce50867fb77cc7ed238bef",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "d7ec495c5f58bd4b22e9225a18398c3dcdacf9e275ce50867fb77cc7ed238bef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "from collections import defaultdict \n",
    "import random\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\utils\")\n",
    "    sys.path.append(module_path+\"\\\\models\")\n",
    "\n",
    "import vectorizer\n",
    "from decision_tree import BinaryDecisionTree\n",
    "import evaluation"
   ]
  },
  {
   "source": [
    "## Data processing functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifier for the last used feature set, used for convenience, added to the file name of the results\n",
    "used_feature_set = \"\"\n",
    "\n",
    "# all of these functions take in the raw text rows and vocabulary \n",
    "# and outputs all instances formatted as feature vectors x and label vectors y\n",
    "def features1(rows, word_to_index):\n",
    "    \"\"\"\n",
    "    Features: is the word present in any of the two observations or hypothesis\n",
    "    \"\"\"\n",
    "    global used_feature_set\n",
    "    used_feature_set = \"features1\"\n",
    "    # make training instances with the first hypothesis\n",
    "    instances  = [(row[1] + \" \" + row[2] + \" \" + row[3], 1 if row[5] == '1' else 0) for row in rows]\n",
    "\n",
    "    # add training instances with the second hypothesis\n",
    "    instances += [(row[1] + \" \" + row[2] + \" \" + row[4], 1 if row[5] == '2' else 0) for row in rows]\n",
    "\n",
    "    # change the text into sparse incidence vectors\n",
    "    vectorized_instances = [(vectorizer.sparse_incidence_vector(text, word_to_index), label) for (text, label) in instances]\n",
    "\n",
    "    # convert from list of (vector, label) tuples into two separate lists\n",
    "    [x, y] = [list(t) for t in zip(*vectorized_instances)]\n",
    "\n",
    "    return x, y, len(word_to_index)\n",
    "\n",
    "def features2(rows, word_to_index):\n",
    "    \"\"\"\n",
    "    Features: is the word present in both the hypothesis and either of the observations\n",
    "    \"\"\"\n",
    "\n",
    "    global used_feature_set \n",
    "    used_feature_set = \"features2\"\n",
    "\n",
    "    # make training instances with the first hypothesis\n",
    "    instances  = [(row[1] + \" \" + row[2], row[3], 1 if row[5] == '1' else 0) for row in rows]\n",
    "\n",
    "    # add training instances with the second hypothesis\n",
    "    instances += [(row[1] + \" \" + row[2], row[4], 1 if row[5] == '2' else 0) for row in rows]\n",
    "\n",
    "    # change the text into sparse incidence vectors\n",
    "    vectorized_instances = [(vectorizer.sparse_incidence_vector(observations, word_to_index).intersection(vectorizer.sparse_incidence_vector(hypothesis, word_to_index)), label) for (observations, hypothesis, label) in instances]\n",
    "\n",
    "    # convert from list of (vector, label) tuples into two separate lists\n",
    "    [x, y] = [list(t) for t in zip(*vectorized_instances)]\n",
    "\n",
    "    return x, y, len(word_to_index)\n",
    "\n",
    "def features3(rows, word_to_index):\n",
    "    \"\"\"\n",
    "    Features: two separate features for each word - is it present in either of the observations, is it present in the hypothesis\n",
    "    \"\"\"\n",
    "    global used_feature_set \n",
    "    used_feature_set = \"features3\"\n",
    "    \n",
    "    # make training instances with the first hypothesis\n",
    "    instances  = [(row[1] + \" \" + row[2], row[3], 1 if row[5] == '1' else 0) for row in rows]\n",
    "\n",
    "    # add training instances with the second hypothesis\n",
    "    instances += [(row[1] + \" \" + row[2], row[4], 1 if row[5] == '2' else 0) for row in rows]\n",
    "\n",
    "    # change the text into sparse incidence vectors\n",
    "    vectorized_instances = []\n",
    "    for (observations, hypothesis, label) in instances:\n",
    "        obs = vectorizer.sparse_incidence_vector(observations, word_to_index)\n",
    "        hyp = vectorizer.sparse_incidence_vector(hypothesis, word_to_index)\n",
    "        vectorized_instances.append((obs | {f + len(word_to_index) for f in hyp}, label))\n",
    "\n",
    "    # convert from list of (vector, label) tuples into two separate lists\n",
    "    [x, y] = [list(t) for t in zip(*vectorized_instances)]\n",
    "\n",
    "    return x, y, len(word_to_index) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to remove useless features. For the sake of keeping data processing steps separate, works with already calculated feature sets\n",
    "def choose_features_to_remove(x, y, total_feature_amount, threshold = 1):\n",
    "    \"\"\"\n",
    "    Counts up how often each feature occurs and returns a set of all the feature indexes that occur below the threshold\n",
    "    \"\"\"\n",
    "    feature_counts = [0] * total_feature_amount\n",
    "    for instance in x:\n",
    "        for f in instance:\n",
    "            feature_counts[f] += 1\n",
    "    \n",
    "    return {i for i, count in enumerate(feature_counts) if count < threshold}\n",
    "\n",
    "def prune_features(x, y, features_to_remove):\n",
    "    \"\"\"\n",
    "    Removes the features given in the set features_to_remove from x.\n",
    "    Note that since features are stored densely with just their index, there's no point in trying to reindex the other features,\n",
    "    since that wouldn't free up any space.\n",
    "    \"\"\"\n",
    "    x = [features - features_to_remove for features in x]\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def resample_dataset(train_rows, dev_rows, test_story_amount=1000):\n",
    "    \"\"\"\n",
    "    Resamples the training and dev sets based on their story_id fields, so that each story id appears only once among both sets.\n",
    "    Then samples a given amount of stories as the new dev set.\n",
    "\n",
    "    Duplicate stories from the training data (that had the same observations, but different hypothesis) are removed, to make sure that \n",
    "    the training set has none of the stories that are in the dev set.\n",
    "    \"\"\"\n",
    "\n",
    "    # get all unique story ids from both given data sets\n",
    "    stories = {instance[0] for instance in train_rows}\n",
    "    test_stories = {instance[0] for instance in dev_rows}\n",
    "    all_stories = stories | test_stories\n",
    "\n",
    "    # randomly sample new train/dev split among the story ids\n",
    "    test_stories = random.sample(all_stories, k=test_story_amount)\n",
    "    test_stories = {story_id for story_id in test_stories}\n",
    "    train_stories = all_stories - test_stories\n",
    "\n",
    "    new_dev_rows = []\n",
    "    new_train_rows = []\n",
    "\n",
    "    # divide all the data rows between dev and train sets, making sure to only take one row for each unique story_id\n",
    "    for row in train_rows + dev_rows:        \n",
    "        if row[0] in test_stories:\n",
    "            new_dev_rows.append(row)\n",
    "            test_stories -= {row[0]}\n",
    "        elif row[0] in train_stories:\n",
    "            new_train_rows.append(row)\n",
    "            train_stories -= {row[0]}\n",
    "\n",
    "    return new_train_rows, new_dev_rows\n"
   ]
  },
  {
   "source": [
    "## Data loading and processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-3-51d714f5f41a>:39: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  test_stories = random.sample(all_stories, k=test_story_amount)\n",
      "18333\n",
      "18333\n",
      "1000\n",
      "1000\n",
      "36666  instances,  27712  features\n",
      "Pruning  22208  features\n",
      "36666  instances with  5504  features remaining\n",
      "2000  test instances\n"
     ]
    }
   ],
   "source": [
    "feature_removal_threshold = 10\n",
    "\n",
    "rows = vectorizer.parse_and_return_rows('../utils/data/processed_data/train.csv')\n",
    "dev_rows = vectorizer.parse_and_return_rows('../utils/data/processed_data/dev.csv')\n",
    "\n",
    "rows, dev_rows = resample_dataset(rows, dev_rows)\n",
    "\n",
    "print(len({instance[0] for instance in rows}))\n",
    "print(len(rows))\n",
    "\n",
    "print(len({instance[0] for instance in dev_rows}))\n",
    "print(len(dev_rows))\n",
    "\n",
    "\n",
    "vocabulary, vocabulary_length = vectorizer.return_len_and_vocabulary(rows)\n",
    "word_to_index = vectorizer.create_token_index(vocabulary)\n",
    "x, y, total_feature_amount = features3(rows, word_to_index)\n",
    "print(len(x), \" instances, \", total_feature_amount, \" features\")\n",
    "features_to_remove = choose_features_to_remove(x, y, total_feature_amount, feature_removal_threshold)\n",
    "print(\"Pruning \", len(features_to_remove), \" features\")\n",
    "x, y = prune_features(x, y, features_to_remove)\n",
    "print(len(x), \" instances with \", total_feature_amount-len(features_to_remove), \" features remaining\")\n",
    "\n",
    "\n",
    "x_dev, y_dev, _ = features3(dev_rows, word_to_index)\n",
    "x_dev, y_dev = prune_features(x_dev, y_dev, features_to_remove)\n",
    "print(len(x_dev), \" test instances\")"
   ]
  },
  {
   "source": [
    "## Training functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tree(max_depth, subset_size, training_instance_threshold, num_threads=1, print_logs=True, decision_tree=None, logs=[], accuracy_frequency=10):\n",
    "    \"\"\"\n",
    "    Performs training of the decision tree one step at a time, while recording various statistics about the process.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # the decision tree instance can be passed as a parameter. \n",
    "    # This is so that training can be stopped manually without losing the trained decision tree instance.\n",
    "    if decision_tree is None:\n",
    "        decision_tree = BinaryDecisionTree()\n",
    "    decision_tree.initialize_training(x, y)\n",
    "\n",
    "    can_keep_expanding = True\n",
    "\n",
    "    while can_keep_expanding:\n",
    "        if max_depth is not None and decision_tree.current_depth >= max_depth:\n",
    "            return logs, decision_tree # max depth reached, stop training early\n",
    "        \n",
    "        one_layer_start = time.time()\n",
    "\n",
    "        # expand the next depth layer of the tree\n",
    "        can_keep_expanding = decision_tree.expand_tree(subset_size, training_instance_threshold, num_threads)\n",
    "\n",
    "        one_layer_end = time.time()\n",
    "        \n",
    "        layer_time = one_layer_end - one_layer_start\n",
    "        total_time = one_layer_end - start\n",
    "\n",
    "        if print_logs:\n",
    "            print(\"Depth: \", decision_tree.current_depth, \"Total nodes: \", decision_tree.total_nodes, \"Time taken on layer: \", layer_time, \"Total time taken: \", total_time)\n",
    "        logs.append((decision_tree.current_depth, decision_tree.total_nodes, layer_time, total_time))\n",
    "\n",
    "        # Calculating accuracy (especially on the training set) is a bit expensive, so we're not doing it every step.\n",
    "        # This is only meant to check up on the progress, since the final accuracy will be calcuated at all depths simultaneously after training is done.\n",
    "        if decision_tree.current_depth % accuracy_frequency == 0:\n",
    "            print(\"dev: \", calculate_accuracy(decision_tree, x_dev, y_dev))\n",
    "            print(\"train: \", calculate_accuracy(decision_tree, x, y))\n",
    "\n",
    "    return logs, decision_tree\n",
    "\n",
    "def calculate_accuracy(decision_tree, x, y):\n",
    "    \"\"\"\n",
    "    Make a prediction on each instance in x with the given decision_tree, then calculate accuracy by comparing predictions to the labels in y.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for instance in x:\n",
    "        predictions.append(decision_tree.predict_class(instance))\n",
    "    return evaluation.calculate_accuracy(predictions, y)\n",
    "\n",
    "def calculate_accuracy_at_all_depths(decision_tree, x, y):\n",
    "    \"\"\"\n",
    "    Make predictions on all instances in x with the given decision_tree at each depth.\n",
    "    Returns a list of accuracies, where accuracies[i] is the accuracy that would be obtained if traversing the tree was stopped when it reached a depth of i.\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "\n",
    "    # make predictions on each instance of x. We get a list of lists of size len(x) * depth\n",
    "    predictions_at_all_depths = decision_tree.predict_classes_at_all_depths(x)\n",
    "\n",
    "    # calculate transpose of predictions, so that it becomes depth * len(x)\n",
    "    predictions_at_all_depths = list(map(list, zip(*predictions_at_all_depths)))\n",
    "\n",
    "    # now when iterating through the predictions each row is a list of predicted classes for all instances in x\n",
    "    for predictions_at_depth in predictions_at_all_depths:\n",
    "        accuracy = evaluation.calculate_accuracy(predictions_at_depth, y)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "def save_results(logs, train_accuracies, dev_accuracies, file_name):\n",
    "    \"\"\"\n",
    "    Saves training statistics in a file with the given name.\n",
    "    \"\"\"\n",
    "    file = open(file_name, \"w\")\n",
    "    file.write(\"acc_train,acc_dev,depth,nodes,time_for_layer,total_time\\n\")\n",
    "    for i, log in enumerate(logs):\n",
    "        message = \"%s,%s,%s,%s,%s,%s\\n\" % (train_accuracies[i], dev_accuracies[i], log[0], log[1], log[2], log[3])\n",
    "        file.write(message)\n",
    "    file.close()\n",
    "\n",
    "def do_experiment(max_depth, subset_size, training_instance_threshold, result_file_name, num_threads=1, print_logs=True, accuracy_frequency=10, tree=None, logs=[]):\n",
    "    \"\"\"\n",
    "    Combines all the previous functions in a self contained test. Trains decision tree, calculates accuracies and saves the results.\n",
    "\n",
    "    max_depth: when the tree reaches this depth, training will be stopped\n",
    "    subset_size: how many training instances to look at when calculating information depth. Using only a subset speeds up training at the cost of accuracy.\n",
    "    training_instance_threshold: normalizing measure. Don't split leaf nodes that have less than this amount of training instances associated with them.\n",
    "    result_file_name: file name in which to store results\n",
    "    num_threads: how many processes to use when calculating information gain. 5 works well on my computer, but will depend on hardware.\n",
    "    accuracy_frequency: how often to calculate the accuracy and print it out. Only used for checking on the progress of the tree, full accuracy will be                                 calculated at the end anyway.\n",
    "    tree, logs: in case the training is stopped before it's finished, if you passed your own tree/logs instances here, you'll still be able to access them \n",
    "                so the training progress won't be lost.\n",
    "    \"\"\"\n",
    "    logs, tree = train_tree(max_depth, subset_size, training_instance_threshold, num_threads, print_logs, tree, logs, accuracy_frequency)\n",
    "                            \n",
    "    print(\"Calculating accuracy at all depths...\")\n",
    "    start = time.time()\n",
    "    train_accuracies = calculate_accuracy_at_all_depths(tree, x, y)\n",
    "    dev_accuracies = calculate_accuracy_at_all_depths(tree, x_dev, y_dev)\n",
    "    end = time.time()\n",
    "    print(\"Time taken: \", end - start)\n",
    "    print(\"Final train accuracy: \", train_accuracies[-1])\n",
    "    print(\"Final dev accuracy: \", dev_accuracies[-1])\n",
    "    save_results(logs, train_accuracies, dev_accuracies, result_file_name)\n",
    "    return logs, tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Depth:  1 Total nodes:  3 Time taken on layer:  22.98327398300171 Total time taken:  22.98627543449402\n",
      "Depth:  2 Total nodes:  7 Time taken on layer:  22.632786512374878 Total time taken:  45.6190619468689\n",
      "Depth:  3 Total nodes:  13 Time taken on layer:  25.264089822769165 Total time taken:  70.88315176963806\n",
      "Depth:  4 Total nodes:  19 Time taken on layer:  29.01190686225891 Total time taken:  99.89605855941772\n",
      "Depth:  5 Total nodes:  29 Time taken on layer:  30.558051824569702 Total time taken:  130.45411038398743\n",
      "Depth:  6 Total nodes:  41 Time taken on layer:  28.565892457962036 Total time taken:  159.02200078964233\n",
      "Depth:  7 Total nodes:  57 Time taken on layer:  28.898897647857666 Total time taken:  187.9208984375\n",
      "Depth:  8 Total nodes:  77 Time taken on layer:  28.84203290939331 Total time taken:  216.76393055915833\n",
      "Depth:  9 Total nodes:  93 Time taken on layer:  28.842495918273926 Total time taken:  245.60742735862732\n",
      "Depth:  10 Total nodes:  109 Time taken on layer:  28.76121473312378 Total time taken:  274.3826460838318\n",
      "dev:  54.19354838709678\n",
      "train:  54.07207634402229\n",
      "Depth:  11 Total nodes:  127 Time taken on layer:  27.26982569694519 Total time taken:  301.7844817638397\n",
      "Depth:  12 Total nodes:  147 Time taken on layer:  26.80414605140686 Total time taken:  328.5906252861023\n",
      "Depth:  13 Total nodes:  169 Time taken on layer:  28.11634063720703 Total time taken:  356.7079665660858\n",
      "Depth:  14 Total nodes:  191 Time taken on layer:  26.578721046447754 Total time taken:  383.28668761253357\n",
      "Depth:  15 Total nodes:  213 Time taken on layer:  26.651520013809204 Total time taken:  409.9382076263428\n",
      "Depth:  16 Total nodes:  235 Time taken on layer:  26.327176809310913 Total time taken:  436.2663834095001\n",
      "Depth:  17 Total nodes:  257 Time taken on layer:  26.463622093200684 Total time taken:  462.7300055027008\n",
      "Depth:  18 Total nodes:  281 Time taken on layer:  26.177161693572998 Total time taken:  488.908166885376\n",
      "Depth:  19 Total nodes:  303 Time taken on layer:  24.612499237060547 Total time taken:  513.5216636657715\n",
      "Depth:  20 Total nodes:  325 Time taken on layer:  25.209571838378906 Total time taken:  538.7322354316711\n",
      "dev:  55.21505376344086\n",
      "train:  55.708019678738665\n",
      "Depth:  21 Total nodes:  349 Time taken on layer:  24.794028997421265 Total time taken:  563.7212762832642\n",
      "Depth:  22 Total nodes:  377 Time taken on layer:  24.94627809524536 Total time taken:  588.6675543785095\n",
      "Depth:  23 Total nodes:  403 Time taken on layer:  25.17927312850952 Total time taken:  613.8478252887726\n",
      "Depth:  24 Total nodes:  429 Time taken on layer:  24.281474351882935 Total time taken:  638.1292996406555\n",
      "Depth:  25 Total nodes:  451 Time taken on layer:  26.006110429763794 Total time taken:  664.1364099979401\n",
      "Depth:  26 Total nodes:  475 Time taken on layer:  23.774972677230835 Total time taken:  687.9113826751709\n",
      "Depth:  27 Total nodes:  499 Time taken on layer:  24.577056884765625 Total time taken:  712.4894409179688\n",
      "Depth:  28 Total nodes:  525 Time taken on layer:  24.22302508354187 Total time taken:  736.7124660015106\n",
      "Depth:  29 Total nodes:  553 Time taken on layer:  25.45661687850952 Total time taken:  762.1700792312622\n",
      "Depth:  30 Total nodes:  581 Time taken on layer:  24.80561661720276 Total time taken:  786.9766938686371\n",
      "dev:  56.075268817204304\n",
      "train:  56.863849208701325\n",
      "Depth:  31 Total nodes:  611 Time taken on layer:  24.459064245224 Total time taken:  811.7432878017426\n",
      "Depth:  32 Total nodes:  639 Time taken on layer:  24.78153109550476 Total time taken:  836.5248188972473\n",
      "Depth:  33 Total nodes:  665 Time taken on layer:  25.185219764709473 Total time taken:  861.7110390663147\n",
      "Depth:  34 Total nodes:  689 Time taken on layer:  23.88204002380371 Total time taken:  885.5940794944763\n",
      "Depth:  35 Total nodes:  713 Time taken on layer:  24.221534967422485 Total time taken:  909.8156144618988\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-326de8856e3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"../experiment_results/decision_tree/%s_%s_prune%s_subset%s_threshold%s.csv\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mused_feature_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_removal_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_instance_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_instance_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_file_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0maccuracy_frequency\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-4ec1a41c834c>\u001b[0m in \u001b[0;36mdo_experiment\u001b[1;34m(max_depth, subset_size, training_instance_threshold, result_file_name, num_threads, print_logs, accuracy_frequency, tree, logs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdo_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_instance_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_logs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_frequency\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_instance_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Calculating accuracy at all depths...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-4ec1a41c834c>\u001b[0m in \u001b[0;36mtrain_tree\u001b[1;34m(max_depth, subset_size, training_instance_threshold, num_threads, print_logs, decision_tree, logs, accuracy_frequency)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mone_layer_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mcan_keep_expanding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_instance_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mone_layer_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Emkin\\Documents\\Uni stuttgart\\SoSe2021\\Team Lab\\nlp-teamlab\\models\\decision_tree.py\u001b[0m in \u001b[0;36mexpand_tree\u001b[1;34m(self, subset_for_information_gain_calculation, training_instance_threshold, num_threads)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtraining_instance_threshold\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtraining_instance_threshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_for_information_gain_calculation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_leaf_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Emkin\\Documents\\Uni stuttgart\\SoSe2021\\Team Lab\\nlp-teamlab\\models\\decision_tree.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, subset_for_information_gain_calculation, num_threads)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_for_information_gain_calculation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;34m\"\"\"Split this into two on the feature that would lead to the biggest information gain\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0minfo_gain_for_all_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minformation_gain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malready_used_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset_for_information_gain_calculation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset_for_information_gain_calculation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# if no possible split leads to an information gain, don't split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_gain_for_all_features\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_gain_for_all_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Emkin\\Documents\\Uni stuttgart\\SoSe2021\\Team Lab\\nlp-teamlab\\models\\decision_tree.py\u001b[0m in \u001b[0;36minformation_gain\u001b[1;34m(x, y, feature_mask, subset_for_information_gain_calculation, num_threads)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;31m# calculate information gain for each set of parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minformation_gain_for_subset_of_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;31m# combine the results of each separate process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         '''\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tree = BinaryDecisionTree()\n",
    "logs = []\n",
    "\n",
    "identifier = \"e10_resampled\"\n",
    "max_depth = 150\n",
    "subset_size = None\n",
    "training_instance_threshold = 100\n",
    "\n",
    "file_name = \"../experiment_results/decision_tree/%s_%s_prune%s_subset%s_threshold%s.csv\" % (identifier, used_feature_set, feature_removal_threshold, subset_size, training_instance_threshold)\n",
    "\n",
    "logs, tree = do_experiment(max_depth, subset_size, training_instance_threshold, result_file_name=file_name, num_threads=5,  accuracy_frequency=10, tree=tree, logs=logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to save the results of a run that was cancelled early  \n",
    "print(\"Calculating accuracy at all depths...\")\n",
    "start = time.time()\n",
    "train_accuracies = calculate_accuracy_at_all_depths(tree, x, y)\n",
    "dev_accuracies = calculate_accuracy_at_all_depths(tree, x_dev, y_dev)\n",
    "end = time.time()\n",
    "print(\"Time taken: \", end - start)\n",
    "print(\"Final train accuracy: \", train_accuracies[-1])\n",
    "print(\"Final dev accuracy: \", dev_accuracies[-1])\n",
    "save_results(logs, train_accuracies, dev_accuracies, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}